# ğŸ§  Day 08 : Neural Networks & Classification

A comprehensive guide to implementing perceptrons and artificial neural networks for binary classification using Python, scikit-learn, and TensorFlow.

---

## ğŸ“š Two Implementation Files

### 1ï¸âƒ£ **Perceptron.ipynb** - Binary Perceptron Classifier

#### ğŸ¯ What it does:

Implements a **basic perceptron from scratch** for binary classification using synthetic blobs dataset with decision boundary visualization.

#### ğŸ”‘ Key Steps:

```
ğŸ“¥ Generate Data: Binary classification blobs (150 samples, 2 features)
     â†“
âœ‚ï¸ Split Data: 80% train, 20% test
     â†“
ğŸ¨ Visualize: Scatter plot of training data
     â†“
ğŸ¤– Build Perceptron: Custom class with unit step activation
     â†“
ğŸ“ Train Model: Iterative weight updates (1000 iterations)
     â†“
ğŸ¯ Make Predictions: Classify test samples
     â†“
ğŸ“Š Evaluate: Calculate accuracy on test set
     â†“
ğŸ“ˆ Visualize: Decision boundary line
```

#### ğŸ’¡ Quick Memory:

- **Foundation of Neural Networks** ğŸ—ï¸
- Hand-coded from scratch (no frameworks)
- Single hidden layer (implicit)
- Unit step activation function
- Shows how neurons learn decision boundaries

#### ğŸ”¢ Key Components:

```python
class Perceptron:
    - weights: Learn pattern strength for each feature
    - bias: Shift decision boundary
    - learning_rate: Controls update step size (0.01)
    - n_iters: Number of training iterations (1000)
```

#### ğŸ”§ Training Algorithm:

```
For each iteration:
    For each training sample (x, y):
        1. Calculate linear output: xÂ·w + b
        2. Apply unit step: if output â‰¥ 0 then 1, else 0
        3. Calculate error: (actual - predicted)
        4. Update weights: w = w + lr Ã— error Ã— x
        5. Update bias: b = b + lr Ã— error
```

#### ğŸ“Š Activation Function:

$$f(x) = \begin{cases} 1 & \text{if } x \geq 0 \\ 0 & \text{if } x < 0 \end{cases}$$

#### ğŸ”¢ Key Variables:

- `X` â†’ Feature data (n_samples Ã— 2)
- `y` â†’ Binary labels (0 or 1)
- `X_train, X_test` â†’ Split features
- `p` â†’ Perceptron instance
- `predictions` â†’ Model output on test set
- `accuracy` â†’ Classification accuracy (%)

---

### 2ï¸âƒ£ **ANN.ipynb** - Artificial Neural Network (TensorFlow/Keras)

#### ğŸ¯ What it does:

Implements a **deep neural network** for bank customer churn prediction using TensorFlow/Keras on real-world data with confusion matrix evaluation.

#### ğŸ”‘ Key Steps:

```
ğŸ“¥ Load Dataset: Churn_Modelling.csv (10,000 customers)
     â†“
ğŸ” Select Features: Extract columns 3 to -1 (skip ID, surname)
     â†“
ğŸ·ï¸ Encode Categorical Data:
   - Label encode 'Gender' (M/F â†’ 0/1)
   - One-hot encode 'Geography' (France/Germany/Spain â†’ 3 columns)
     â†“
ğŸ“Š Prepare Data: Convert to numpy arrays
     â†“
âœ‚ï¸ Split Data: 80% train, 20% test
     â†“
ğŸ“ Feature Scaling: StandardScaler normalization
     â†“
ğŸ—ï¸ Build ANN Architecture:
   - Input Layer: (implicit, 12 features)
   - Hidden Layer 1: 6 neurons + ReLU activation
   - Hidden Layer 2: 6 neurons + ReLU activation
   - Output Layer: 1 neuron + Sigmoid activation
     â†“
âš™ï¸ Compile Model: Adam optimizer + Binary Crossentropy
     â†“
ğŸ“ Train: 100 epochs, batch size 32
     â†“
ğŸ”® Make Predictions: Single customer & test set
     â†“
ğŸ“Š Evaluate: Confusion matrix & accuracy score
```

#### ğŸ’¡ Quick Memory:

- **Production-Ready Deep Learning** ğŸš€
- Uses TensorFlow/Keras (industry standard)
- Multiple hidden layers (deep)
- ReLU + Sigmoid activation functions
- Real customer dataset (10,000 samples)
- Complete ML pipeline with preprocessing

#### ğŸ—ï¸ Network Architecture:

```
Input Features (12)
        â†“
   [Dense 6 neurons]
   ReLU activation
        â†“
   [Dense 6 neurons]
   ReLU activation
        â†“
   [Dense 1 neuron]
   Sigmoid activation
        â†“
   Output: Churn probability (0-1)
```

#### ğŸ”§ Data Preprocessing Pipeline:

```python
1. Load CSV â†’ DataFrame
2. Feature Selection â†’ X[:, 3:-1], y[:, -1]
3. Label Encoding â†’ Gender column
4. One-Hot Encoding â†’ Geography column
5. Train/Test Split â†’ 80/20
6. Feature Scaling â†’ Standardize to mean=0, std=1
```

#### ğŸ“Š Activation Functions:

**Hidden Layers (ReLU):**
$$f(x) = \begin{cases} x & \text{if } x > 0 \\ 0 & \text{otherwise} \end{cases}$$

**Output Layer (Sigmoid):**
$$f(x) = \frac{1}{1 + e^{-x}}$$

#### ğŸ”¢ Key Variables:

- `df` â†’ Loaded CSV dataset
- `x, y` â†’ Features and target
- `x_train, x_test, y_train, y_test` â†’ Split data
- `sc` â†’ StandardScaler object
- `ann` â†’ Sequential model instance
- `y_pred` â†’ Model predictions on test set
- `cm` â†’ Confusion matrix
- `accuracy` â†’ Classification accuracy (%)

#### ğŸ“‹ Model Compilation:

```python
ann.compile(
    optimizer='adam',          # Adaptive learning rate optimizer
    loss='binary_crossentropy', # For binary classification
    metrics=['accuracy']        # Monitor accuracy
)
```

#### ğŸ“ Training Configuration:

```python
ann.fit(
    x_train, y_train,
    batch_size=32,    # Process 32 samples per update
    epochs=100        # Train 100 times through dataset
)
```

#### ğŸ“Š Example Prediction:

```
Input: Customer from France, Credit Score 600, Male, 40 years old,
       3 years tenure, $60,000 balance, 2 products, Credit Card: Yes,
       Active Member: Yes, Salary: $50,000

Output: Probability > 0.5 â†’ Customer will CHURN âŒ
        Probability â‰¤ 0.5 â†’ Customer will STAY âœ…
```

---

## ğŸ”„ Side-by-Side Comparison

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature          â”‚ Perceptron.ipynb   â”‚ ANN.ipynb            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Library          â”‚ NumPy (scratch)    â”‚ TensorFlow/Keras     â”‚
â”‚ Dataset          â”‚ Synthetic blobs    â”‚ Real bank data       â”‚
â”‚ Samples          â”‚ 150                â”‚ 10,000               â”‚
â”‚ Features         â”‚ 2                  â”‚ 12                   â”‚
â”‚ Hidden Layers    â”‚ 0 (single neuron)  â”‚ 2 (6 + 6 neurons)    â”‚
â”‚ Activation Fn    â”‚ Unit Step          â”‚ ReLU + Sigmoid       â”‚
â”‚ Train/Test Split â”‚ âœ… 80/20           â”‚ âœ… 80/20             â”‚
â”‚ Preprocessing    â”‚ âŒ None            â”‚ âœ… Scaling + Encodingâ”‚
â”‚ Data Type        â”‚ 2D numeric         â”‚ Mixed (categorical)  â”‚
â”‚ Visualization    â”‚ âœ… Decision line   â”‚ âŒ None (complex)    â”‚
â”‚ Metrics          â”‚ Accuracy only      â”‚ Confusion Matrix     â”‚
â”‚ Training Time    â”‚ âš¡ Instant         â”‚ â±ï¸ Several seconds   â”‚
â”‚ Use Case         â”‚ Learning basics    â”‚ Production system    â”‚
â”‚ Complexity       â”‚ â­ Beginner        â”‚ â­â­â­ Advanced      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Complete Machine Learning Workflow

### ğŸ”„ Perceptron Workflow

```python
# 1. Import & Generate Data
import numpy as np
from sklearn.model_selection import train_test_split
X, y = datasets.make_blobs(n_samples=150, n_features=2, centers=2)

# 2. Split Data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=123
)

# 3. Initialize & Train
p = Perceptron(learning_rate=0.01, n_iters=1000)
p.fit(X_train, y_train)

# 4. Predict
predictions = p.predict(X_test)

# 5. Evaluate
accuracy = np.sum(y_test == predictions) / len(y_test)
print(f"Accuracy: {accuracy * 100:.2f}%")

# 6. Visualize
plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train)
plt.plot([x0_1, x0_2], [x1_1, x1_2], 'k')  # Decision boundary
plt.show()
```

### ğŸ”„ ANN Workflow

```python
# 1. Import & Load Data
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
df = pd.read_csv('Churn_Modelling.csv')

# 2. Preprocessing
# - Extract features & target
# - Encode categorical variables (Label + One-Hot)
# - Train/Test split
# - Feature scaling

# 3. Build Network
ann = tf.keras.models.Sequential([
    tf.keras.layers.Dense(6, activation='relu'),
    tf.keras.layers.Dense(6, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# 4. Compile
ann.compile(optimizer='adam', loss='binary_crossentropy',
            metrics=['accuracy'])

# 5. Train
ann.fit(x_train, y_train, batch_size=32, epochs=100)

# 6. Predict
y_pred = ann.predict(x_test)
y_pred = (y_pred > 0.5)  # Convert probability to binary

# 7. Evaluate
from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")
```

---

## ğŸ“Š Performance Comparison

| Aspect                  | Perceptron   | ANN                     |
| ----------------------- | ------------ | ----------------------- |
| **Speed**               | âš¡ Very Fast | â±ï¸ Moderate             |
| **Accuracy**            | ğŸ“Š ~80-90%   | ğŸ“Š ~85-95%              |
| **Complexity**          | ğŸ“ Simple    | ğŸš€ Complex              |
| **Interpretability**    | ğŸ” Easy      | ğŸ” Hard                 |
| **Real-world**          | âŒ Rarely    | âœ… Often                |
| **Scaling**             | âŒ Limited   | âœ… Excellent            |
| **Feature Engineering** | âŒ Manual    | âœ… Learns automatically |

---

## ğŸ”‘ Key Concepts

### Perceptron Concepts

1. **Weight Updates**: $w_i = w_i + \alpha \times (y - \hat{y}) \times x_i$
2. **Bias Update**: $b = b + \alpha \times (y - \hat{y})$
3. **Decision Boundary**: Linear line separating classes
4. **Convergence**: Reaches stable weights after iterations

### ANN Concepts

1. **Forward Propagation**: Input â†’ Hidden layers â†’ Output
2. **Backpropagation**: Calculate error gradients
3. **Gradient Descent**: Optimize weights iteratively
4. **Regularization**: Prevent overfitting
5. **Batch Processing**: Update weights after multiple samples

---

## ğŸ’¡ When to Use What?

| Scenario                      | Use This                   |
| ----------------------------- | -------------------------- |
| ğŸ“š Learning neural networks   | `Perceptron.ipynb`         |
| ğŸ“ Understanding fundamentals | `Perceptron.ipynb`         |
| ğŸ¢ Production ML system       | `ANN.ipynb`                |
| ğŸ”® Complex data patterns      | `ANN.ipynb`                |
| ğŸ“Š Binary classification      | Both (choose by data size) |
| âš¡ Need fast predictions      | `Perceptron.ipynb`         |
| ğŸ¯ Maximize accuracy          | `ANN.ipynb`                |
| ğŸ” Interpretable model        | `Perceptron.ipynb`         |

---

## ğŸ“ˆ Typical Accuracy Results

```
Perceptron on synthetic data:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 85% âœ“

ANN on bank churn data:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 86% âœ“
```

---

## ğŸ”§ Prerequisites & Setup

```python
# Required Libraries
import pandas as pd          # Data manipulation
import numpy as np          # Numerical computing
import matplotlib.pyplot as plt  # Visualization
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.metrics import confusion_matrix, accuracy_score
import tensorflow as tf     # Deep learning (ANN only)
```

### Installation (if needed):

```bash
pip install pandas numpy matplotlib scikit-learn tensorflow
```

---

## ğŸ¯ Learning Path

```
Start Here
    â†“
1ï¸âƒ£ Perceptron.ipynb
   âœ… Understand neurons
   âœ… Learn decision boundaries
   âœ… See training in action
    â†“
2ï¸âƒ£ ANN.ipynb
   âœ… Build deeper networks
   âœ… Handle real data
   âœ… Apply preprocessing
   âœ… Use industry tools
    â†“
Advanced Topics
   â†’ Convolutional Neural Networks (CNN)
   â†’ Recurrent Neural Networks (RNN)
   â†’ Hyperparameter tuning
   â†’ Model deployment
```

---

## âœ… Key Takeaways

1. ğŸ§  **Neurons mimic brain learning** through weight adjustments
2. ğŸ“ **Perceptrons** are single-layer, good for learning
3. ğŸš€ **ANNs** are multi-layer, powerful for complex problems
4. ğŸ“Š **Preprocessing matters** (scaling, encoding, splitting)
5. ğŸ¯ **Activation functions** add non-linearity and flexibility
6. ğŸ“ˆ **More layers â‰  Always better** (depends on data)
7. âš™ï¸ **Hyperparameters** significantly affect performance
8. ğŸ“‹ **Evaluation metrics** help choose the best model

---

## ğŸš€ Quick Start

**For Perceptron**:

1. Open `Perceptron.ipynb`
2. Run cells top to bottom
3. Watch decision boundary form
4. Understand weight updates

**For ANN**:

1. Ensure `Churn_Modelling.csv` is in same directory
2. Open `ANN.ipynb`
3. Run preprocessing cells
4. Train the network
5. Check confusion matrix & accuracy

---

## ğŸ“š Model Comparison Summary

```
Perceptron vs ANN

Input (2 features)
    â†“
Perceptron:        ANN:
[w1, w2]           [Dense 6]
   +b    (0 hl)       +ReLU
 sigmoid            [Dense 6]
   â†“                  +ReLU
Output             [Dense 1]
(binary)           +Sigmoid
                      â†“
                   Output
                   (binary)

Simple             Complex
Fast               Slower
80%+ accuracy      85%+ accuracy
```

---

**Created for EL 4152 - Machine Learning | Day 08 Practicals** ğŸ“

_Understanding both simple (Perceptron) and complex (ANN) approaches is crucial for mastering neural networks!_



























